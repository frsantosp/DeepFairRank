{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-04-02T13:55:23.778238Z",
     "start_time": "2024-04-02T13:55:21.926701Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from collections import OrderedDict\n",
    "from codes.metrics import *\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class DNN(torch.nn.Module):\n",
    "    def __init__(self, layers, dropout=0.1):\n",
    "        super(DNN, self).__init__()\n",
    "\n",
    "        # parameters\n",
    "        self.depth = len(layers) - 1\n",
    "\n",
    "        # set up layer order dict\n",
    "        self.activation = torch.nn.Tanh\n",
    "\n",
    "        layer_list = list()\n",
    "        for i in range(self.depth - 1):\n",
    "            layer_list.append(\n",
    "                ('layer_%d' % i, torch.nn.Linear(layers[i], layers[i+1],bias=True))\n",
    "            )\n",
    "            layer_list.append(('activation_%d' % i, self.activation()))\n",
    "            layer_list.append(('dropout_%d' % i,torch.nn.Dropout(dropout)))\n",
    "\n",
    "        layer_list.append(\n",
    "            ('layer_%d' % (self.depth - 1), torch.nn.Linear(layers[-2], layers[-1]))\n",
    "        )\n",
    "\n",
    "        # deploy layers\n",
    "        self.layers = torch.nn.Sequential(OrderedDict(layer_list))\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layers(x)\n",
    "        return out\n",
    "    \n",
    "def training(model,optimizer, max_epoch,B_0,B_1,bin_width,numBins,W,sigma,alpha,beta, print_epoch =50):\n",
    "    for epoch in range(max_epoch+1):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        h_pred = F.sigmoid(model(s))\n",
    "        wsp = weighted_statistical_disparity_torch( h_pred.reshape(1,-1).t(), B_0, B_1, bin_width)\n",
    "\n",
    "        loss = sigma*torch.sum((h_pred - s)**2) + alpha*wsp + beta*(torch.sum(torch.mm(W,h_pred.reshape(1,-1).t()))/W.shape[0])\n",
    "        \n",
    "        if epoch % print_epoch == 0 :\n",
    "            result = evaluate(h_pred.detach().numpy(), y, s, prt, W_, numBins = numBins, bin_width = bin_width,print_flag=0)\n",
    "            print(\"Epoch: %5d  || Loss: %.4f || prec: %.6f |  Rank corr: %.6f  | fp: %.6f | wsd: %.6f | wrd: %.6f )\" % (epoch, loss.item(), result['precision'], result['corr'], result['fairperception'], result['wsd'] ,result['wrd']))\n",
    "            \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    return h_pred.detach().numpy(), evaluate(h_pred.detach().numpy(), y, s, prt, W_,B_0=B_0.numpy(),B_1=B_1.numpy(), numBins = numBins, bin_width = bin_width,print_flag=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[2], line 32\u001B[0m\n\u001B[1;32m     29\u001B[0m filename \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdata/\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;241m+\u001B[39m data \n\u001B[1;32m     30\u001B[0m output_file \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m./results/\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;241m+\u001B[39m data \n\u001B[0;32m---> 32\u001B[0m adj, data, W_, s, y, prt \u001B[38;5;241m=\u001B[39m \u001B[43mload_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilename\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mclass_attr\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mscore_attr\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mprt_attr\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mscore_norm\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mscore_norm\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     33\u001B[0m B_0, B_1 \u001B[38;5;241m=\u001B[39m generate_bins_matrix(s,prt,numBins)\n\u001B[1;32m     34\u001B[0m B_0 \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mtensor(B_0)\u001B[38;5;241m.\u001B[39mtype(dtype \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mfloat32)\n",
      "File \u001B[0;32m~/Documents/FairNodeRanking-main/codes/data_loader.py:46\u001B[0m, in \u001B[0;36mload_data\u001B[0;34m(file_name, class_label, score_label, prt_group, score_norm)\u001B[0m\n\u001B[1;32m     44\u001B[0m h \u001B[38;5;241m=\u001B[39m data[class_label]\u001B[38;5;241m.\u001B[39mvalues\n\u001B[1;32m     45\u001B[0m prt_f \u001B[38;5;241m=\u001B[39m data[prt_group]\u001B[38;5;241m.\u001B[39mvalues\n\u001B[0;32m---> 46\u001B[0m W \u001B[38;5;241m=\u001B[39m \u001B[43mW_gen\u001B[49m\u001B[43m(\u001B[49m\u001B[43madj\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43ms\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreshape\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     48\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m adj, data, W, s\u001B[38;5;241m.\u001B[39mravel(), h, prt_attr\n",
      "File \u001B[0;32m~/Documents/FairNodeRanking-main/codes/data_loader.py:14\u001B[0m, in \u001B[0;36mW_gen\u001B[0;34m(adj, s)\u001B[0m\n\u001B[1;32m     12\u001B[0m A \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39meye(adj\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m])\u001B[38;5;241m+\u001B[39madj\n\u001B[1;32m     13\u001B[0m B \u001B[38;5;241m=\u001B[39m K\u001B[38;5;241m*\u001B[39mA\n\u001B[0;32m---> 14\u001B[0m W \u001B[38;5;241m=\u001B[39m \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdiag\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[38;5;241;43m/\u001B[39;49m\u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msum\u001B[49m\u001B[43m(\u001B[49m\u001B[43mB\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxis\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdot\u001B[49m\u001B[43m(\u001B[49m\u001B[43mB\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     15\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m W\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from fairsearchcore.models import FairScoreDoc\n",
    "import fairsearchcore as fsc\n",
    "#from codes.alg import FPRank, FSPR_model, best_FPRank\n",
    "from codes.metrics import evaluate\n",
    "from codes.data_loader import load_data\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "######################################################################\n",
    "# Column names\n",
    "######################################################################\n",
    "\n",
    "data = 'credit_married_2'\n",
    "prt_attr = 'Married'\n",
    "class_attr = 'h_c'\n",
    "score_attr = 's'\n",
    "\n",
    "######################################################################\n",
    "# Configuration parameters\n",
    "######################################################################\n",
    "\n",
    "numBins = 10\n",
    "bin_width = 0.1\n",
    "score_norm = 1\n",
    "\n",
    "filename = 'data/' + data \n",
    "output_file = './results/' + data \n",
    "\n",
    "adj, data, W_, s, y, prt = load_data(filename, class_attr, score_attr, prt_attr, score_norm = score_norm)\n",
    "B_0, B_1 = generate_bins_matrix(s,prt,numBins)\n",
    "B_0 = torch.tensor(B_0).type(dtype = torch.float32)\n",
    "B_1 = torch.tensor(B_1).type(dtype = torch.float32)\n",
    "s = torch.tensor(s).type(dtype = torch.float32)\n",
    "W = torch.tensor(W_).type(dtype = torch.float32) - torch.eye(adj.shape[0])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T13:59:44.284760Z",
     "start_time": "2024-04-02T13:55:23.779007Z"
    }
   },
   "id": "df9d2a40b86836b0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "max_epoch = 1000\n",
    "layers = [s.shape[0],256,256,256,s.shape[0]]\n",
    "lr = 0.0001\n",
    "weight_decay = 1e-5\n",
    "alpha_lt = [1,2,3,4,5,10,20,30,40,50,100]\n",
    "beta_lt = [1,2,3,4,5,10,20,30,40,50,100]\n",
    "sigma=1\n",
    "result_lt=[]\n",
    "params = {'alpha': list(), 'beta': list()}\n",
    "results = {'corr': list(), 'precision': list(), 'fairperception': list(), 'wsd': list(), 'wrd': list(), 'h': list()}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T13:59:44.285520Z",
     "start_time": "2024-04-02T13:59:44.285440Z"
    }
   },
   "id": "98dd9e88142e0ce7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for i in range(len(beta_lt)):\n",
    "    alpha = alpha_lt[i]\n",
    "    for j in range(len(alpha_lt)):\n",
    "        beta = beta_lt[j]\n",
    "        params['alpha'].append(alpha)\n",
    "        params['beta'].append(beta)\n",
    "        print(\"Alpha: \", alpha, 'Beta: ', beta)\n",
    "        model = DNN(layers, dropout=0.0)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        h, result = training(model,optimizer, max_epoch,B_0,B_1,bin_width,numBins,W,sigma,alpha,beta)\n",
    "        print(\"Finished! Results: -->|| prec: %.6f |  Rank corr: %.6f  | fp: %.6f | wsd: %.6f | wrd: %.6f )\" % (result['precision'], result['corr'], result['fairperception'], result['wsd'],result['wrd']))\n",
    "        \n",
    "        results['h'].append(h)\n",
    "        results['corr'].append(result['corr'])\n",
    "        results['precision'].append(result['precision'])\n",
    "        results['fairperception'].append(result['fairperception'])\n",
    "        results['wsd'].append(result['wsd'])\n",
    "        results['wrd'].append(result['wrd'])\n",
    "        \n",
    "        result_lt.append(result)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T13:59:44.287126Z",
     "start_time": "2024-04-02T13:59:44.286417Z"
    }
   },
   "id": "970ec4aa55fed1aa"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "precision_lt =[]\n",
    "fair_p =[]\n",
    "wsd_lt = []\n",
    "wrd_lt = []\n",
    "for result in result_lt:\n",
    "    precision_lt.append(result['precision'])\n",
    "    fair_p.append(result['fairperception'])\n",
    "    wsd_lt.append(result['wsd'])\n",
    "    wrd_lt.append(result['wrd'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-02T13:59:44.287783Z"
    }
   },
   "id": "9d5508ba9fbfc2ac"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.plot(precision_lt[:11],wsd_lt[:11])\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-02T13:59:44.289030Z"
    }
   },
   "id": "db3cf4f53b1f526c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.plot(precision_lt[:11],fair_p[:11])\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-02T13:59:44.290415Z"
    }
   },
   "id": "37c4bcb9b7312096"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.plot(precision_lt[11:11+11],wsd_lt[11:11+11])\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-02T13:59:44.291591Z"
    }
   },
   "id": "661aaa9ff5967543"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from codes.alg import *\n",
    "h,best_param,utils = best_FPRank(results, params, criterion = 'pmean', toRank=False)\n",
    "result = evaluate(h, y, s, prt, W_,B_0=B_0.numpy(),B_1=B_1.numpy(), numBins = numBins, bin_width = bin_width,print_flag=0)\n",
    "print(\"|| prec: %.6f |  Rank corr: %.6f  | fp: %.6f | wsd: %.6f | wrd: %.6f )\" % (result['precision'], result['corr'], result['fairperception'], result['wsd'] ,result['wrd']))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-02T13:59:44.292572Z"
    }
   },
   "id": "531b8a6be9536a06"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(results)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-02T13:59:44.293350Z"
    }
   },
   "id": "f74a544e4dd0db1b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "out_df = pd.DataFrame({'h': h, prt_attr : data[prt_attr]})\n",
    "out_df.to_csv(output_file + '_DFR.csv', index = False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-02T13:59:44.294117Z"
    }
   },
   "id": "6f0337ad377cda50"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#best alpha 1 and beta 100"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T13:59:44.295192Z",
     "start_time": "2024-04-02T13:59:44.294990Z"
    }
   },
   "id": "1408dc0f1778dc24"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
