{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairsearchcore.models import FairScoreDoc\n",
    "import fairsearchcore as fsc\n",
    "\n",
    "file_name = 'compas_clean_2.csv'\n",
    "output_file = './data/Baseline/COMPAS_FairTopK.csv'\n",
    "adj_name = 'adj_matrix_2.npy'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate results for FairTopk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def load_data(file_name, class_label, score_label, prt_group):\n",
    "    \"\"\"\n",
    "    Load data in a format compatible with fairsearchcore package for fairtopk algorithm.\n",
    "    \"\"\"\n",
    "    data =  pd.read_csv(file_name)\n",
    "    k = int(np.sum(data[class_label])) #\n",
    "    p = 1-np.sum(data[prt_group])/data.shape[0]\n",
    "    rankings = list()\n",
    "    for idx in data[score_label].argsort()[::-1]:\n",
    "        if data.loc[idx,prt_group] == 1:\n",
    "            rankings.append( FairScoreDoc(idx, data.loc[idx,score_label], False))\n",
    "        if data.loc[idx,prt_group] == 0:\n",
    "            rankings.append( FairScoreDoc(idx, data.loc[idx,score_label], True))\n",
    "    return rankings, k, p, data\n",
    "alpha = 0.1\n",
    "rankings, k, p, data = load_data(file_name, 'h_c', 's', 'race')\n",
    "\n",
    "fair = fsc.Fair(k, p, alpha)\n",
    "re_ranked = fair.re_rank(rankings)\n",
    "eps = 1/len(rankings)\n",
    "h = np.zeros(len(rankings))\n",
    "for i,item in enumerate(re_ranked):\n",
    "    h[item.id] = 1 - i*eps\n",
    "out_df = pd.DataFrame({'h': h, 'race' : data['race']})\n",
    "out_df.to_csv(output_file, index = False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Generate results for ROC(EOD)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "import aif360\n",
    "from aif360.algorithms.postprocessing.calibrated_eq_odds_postprocessing import CalibratedEqOddsPostprocessing\n",
    "from aif360.algorithms.postprocessing.reject_option_classification import RejectOptionClassification\n",
    "from codes.utils import power_mean, binarize\n",
    "\n",
    "def load_data(file_name, class_label, score_label, prt_group):\n",
    "    \"\"\"\n",
    "    Load data in a format compatible with aif360 package.\n",
    "    \"\"\"\n",
    "    data =  pd.read_csv(file_name)\n",
    "    data['s'] = data[score_label]/10\n",
    "    data['s_hat'] = binarize(data['s'], int(np.sum(data[class_label])))\n",
    "    dataset = aif360.datasets.BinaryLabelDataset(\n",
    "        favorable_label=1,\n",
    "        unfavorable_label=0,\n",
    "        df=data,\n",
    "        label_names=['s_hat'],\n",
    "        protected_attribute_names=[prt_group])\n",
    "    dataset.scores = data['s'].to_numpy().reshape((data.shape[0],1))\n",
    "    return dataset, data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_ub = 0.02\n",
    "metric_lb = -0.02\n",
    "\n",
    "dataset, df = load_data(file_name, 'h_c', 's', 'race')\n",
    "ROC = RejectOptionClassification(unprivileged_groups=[{'race': 0}],\n",
    "                                         privileged_groups= [{'race': 1}],\n",
    "                                         low_class_thresh=0.01, high_class_thresh=0.99,\n",
    "                                          num_class_thresh=100, num_ROC_margin=50,\n",
    "                                          metric_name= \"Equal opportunity difference\",\n",
    "                                          metric_ub=metric_ub, metric_lb=metric_lb)\n",
    "dataset_pred = dataset.copy(deepcopy = True)\n",
    "ROC = ROC.fit(dataset, dataset_pred)\n",
    "dataset_transf_pred = ROC.predict(dataset)\n",
    "h = dataset_transf_pred.labels[:,0]\n",
    "out_df = pd.DataFrame({'h': h, 'race' :df['race']})\n",
    "out_df.to_csv('./data/Baseline/Compas_ROC(EOD).csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate results for ROC(SPD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_ub = 0.02\n",
    "metric_lb = -0.02\n",
    "dataset, df = load_data(file_name, 'h_c', 's', 'race')\n",
    "ROC = RejectOptionClassification(unprivileged_groups=[{'race': 0}],\n",
    "                                         privileged_groups= [{'race': 1}],\n",
    "                                         low_class_thresh=0.01, high_class_thresh=0.99,\n",
    "                                          num_class_thresh=100, num_ROC_margin=50,\n",
    "                                          metric_name= \"Statistical parity difference\",\n",
    "                                          metric_ub=metric_ub, metric_lb=metric_lb)\n",
    "dataset_pred = dataset.copy(deepcopy = True)\n",
    "ROC = ROC.fit(dataset, dataset_pred)\n",
    "dataset_transf_pred = ROC.predict(dataset)\n",
    "h = dataset_transf_pred.labels[:,0]\n",
    "out_df = pd.DataFrame({'h': h, 'race' :df['race']})\n",
    "out_df.to_csv('./data/Baseline/Compas_ROC(SPD).csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate results for CEP (FPR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset, df = load_data(file_name, 'h_c', 's', 'race')\n",
    "cpp = CalibratedEqOddsPostprocessing(unprivileged_groups=[{'race': 0}],\n",
    "                                         privileged_groups= [{'race': 1}], cost_constraint='fpr',seed=1)\n",
    "dataset_pred = dataset.copy(deepcopy = True)\n",
    "cpp = cpp.fit(dataset, dataset_pred)\n",
    "dataset_transf_pred = cpp.predict(dataset)\n",
    "h = dataset_transf_pred.labels[:,0]\n",
    "out_df = pd.DataFrame({'h': h, 'race' :df['race']})\n",
    "out_df.to_csv('./data/Baseline/Compas_CPP(FPR).csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
